#!/bin/bash -x

# Log all this script's output to user-data.log. Only for debugging purposes.
exec > >(tee /var/log/user-data.log|logger -t user-data -s 2>/dev/console) 2>&1

# Make sure that ${linux_user} and ubuntu don't need to use their password in order to use sudo
cat << EOF >> /etc/sudoers

${linux_user} ALL=(ALL:ALL) NOPASSWD: ALL
ubuntu ALL=(ALL:ALL) NOPASSWD: ALL
EOF

# Update packages
sudo apt update -y
sudo apt upgrade -y

# Install AWS CLI
mkdir -p /opt/custom
cd /opt/custom
curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
sudo apt install -y unzip
unzip awscliv2.zip
./aws/install --update

# Format and mount secondary drive
echo 'type=83' | sudo sfdisk /dev/nvme1n1
sudo mkfs.xfs /dev/nvme1n1p1
sudo mkdir -p /var/www
sudo mount /dev/nvme1n1p1 /var/www
sudo cp -p /etc/fstab{,.bak}
echo "/dev/nvme1n1p1 /var/www xfs defaults 0 0" | sudo tee -a /etc/fstab

# Format and mount tertiary drive
echo 'type=83' | sudo sfdisk /dev/nvme2n1
sudo mkfs.xfs /dev/nvme2n1p1
sudo mkdir -p /backup
sudo mount /dev/nvme2n1p1 /backup
sudo cp -p /etc/fstab{,.bak}
echo "/dev/nvme2n1p1 /backup xfs defaults 0 0" | sudo tee -a /etc/fstab

# Create and configure secondary user
sudo useradd ${linux_user} -m -s /bin/bash
sudo usermod -aG sudo ${linux_user}

# Create SSH identity for secondary user
sudo su - ${linux_user}
mkdir -p /home/${linux_user}/.ssh
ssh-keygen -t ed25519 -f /home/${linux_user}/.ssh/${linux_user}-key -N ""
cat /home/${linux_user}/.ssh/${linux_user}-key.pub >> /home/${linux_user}/.ssh/authorized_keys
chmod 700 /home/${linux_user}/.ssh
chmod 600 /home/${linux_user}/.ssh/authorized_keys
chmod 400 /home/${linux_user}/.ssh/${linux_user}-key
chmod 400 /home/${linux_user}/.ssh/${linux_user}-key.pub
chown -R ${linux_user}:${linux_user} /home/${linux_user}/.ssh
sudo echo '${linux_password}' | passwd --stdin ${linux_user}

# Save the secondary user's public and private keys in SSM Parameter Store
sudo /usr/local/bin/aws ssm put-parameter --name "/${project_name}/${linux_user}-key-public" --value file:///home/${linux_user}/.ssh/${linux_user}-key.pub --type SecureString --overwrite
sudo /usr/local/bin/aws ssm put-parameter --name "/${project_name}/${linux_user}-key-private" --value file:///home/${linux_user}/.ssh/${linux_user}-key --type SecureString --overwrite

# Save all other credentials in SSM Parameter Store
sudo cat > /tmp/credentials.json << EOF
mysql_db = ${mysql_db}
mysql_user = ${mysql_user}
mysql_password = ${mysql_password}
EOF
sudo /usr/local/bin/aws ssm put-parameter --name "/${project_name}/mysql-credentials" --value file:///tmp/credentials.json --type SecureString --overwrite

sudo cat > /tmp/credentials.json << EOF
admin_email = ${admin_email}
ghost_url = https://${ghost_domain}
EOF
sudo /usr/local/bin/aws ssm put-parameter --name "/${project_name}/ghost-credentials" --value file:///tmp/credentials.json --type SecureString --overwrite

sudo cat > /tmp/credentials.json << EOF
linux_user = ${linux_user}
linux_password = ${linux_password}
EOF
sudo /usr/local/bin/aws ssm put-parameter --name "/${project_name}/linux-credentials" --value file:///tmp/credentials.json --type SecureString --overwrite
sudo rm -f /tmp/credentials.json

# Create /backup/exclude.txt
cat > /backup/exclude.txt << EOF
*.bz
*.bz2
*.bzip
*.bzip2
*.gz
*.gzip
*.log
*_log
*.sock
*.socket
*.tar
*.zip
*sess_*
EOF

# Create cron job that creates a backup of the database and saves a snapshot of the production site under `/backup` directory and email a summary every day at midnight
cat > /opt/custom/backup.sh << EOF
#!/bin/bash

backup_dir="/backup/$(date +%Y-%m-%d)"
db_backup_file="$backup_dir/${mysql_db}.sql.gz"
log_file="/var/log/backup.log"

# Create backup directory
mkdir -p $backup_dir

echo "" > $log_file
echo "---------- Starting MySQL backup ----------" >> $log_file
if mysqldump -u root -p '${mysql_password}' --log-error=$log_file ${mysql_db} 2>$log_file | gzip > $db_backup_file; then
	echo "$(date) - File $db_backup_file successfully created." >> $log_file
	echo "-------------------------------------------" >> $log_file
else
	echo "$(date) - File $db_backup_file could not be created. You should manually backup the database ASAP." >> $log_file
	echo "-------------------------------------------" >> $log_file
fi

echo "" >> $log_file
echo "---------- Starting Ghost directory backup ----------" >> $log_file
if rsync -avz --exclude-from=/backup/exclude.txt /var/www/ghost/ /backup/$(date +%Y-%m-%d)/ghost; then
	echo "$(date) - Ghost directory successfully backed up." >> $log_file
	echo "-------------------------------------------" >> $log_file
else
	echo "$(date) - Ghost directory could not be backed up. You should manually back it up ASAP." >> $log_file
	echo "-------------------------------------------" >> $log_file
fi

# Enforce retention policy
echo "" >> $log_file
echo "---------- Starting retention policy enforcement ----------" >> $log_file
# Delete files older than 7 days
if find /backup -type d -mtime +7 -exec rm -rf {} \; ; then
	echo "$(date) - Old backup files and directories deleted." >> $log_file
	echo "-------------------------------------------" >> $log_file
else
	echo "$(date) - Old backup files and directories could not be deleted. You should manually delete them ASAP." >> $log_file
	echo "-------------------------------------------" >> $log_file
fi

# Send an email with the backup log
aws sns publish --topic-arn "${topic_arn}" --message file://$log_file
EOF
chmod 700 /opt/custom/backup.sh

sudo touch mycron
echo "0 0 * * * /opt/custom/backup.sh >/dev/null 2>&1" | sudo tee -a mycron
sudo crontab mycron
sudo rm -f mycron

# Install NGINX and MySQL
sudo apt install -y nginx mysql-server
sudo systemctl enable --now nginx
sudo systemctl enable --now mysql

# Configure and activate the firewall
sudo ufw allow 'OpenSSH'
sudo ufw allow 'Nginx Full'
yes | sudo ufw enable

# Set MySQL password
sudo mysql -u root -e "ALTER USER 'root'@'localhost' IDENTIFIED WITH mysql_native_password BY '${mysql_password}';"

# Switch to ubuntu user
sudo su - ubuntu

# Add the NodeSource APT repository for Node 16
curl -sL https://deb.nodesource.com/setup_16.x | sudo -E bash

# Install Node.js and development tools in case it's needed to build native addons
sudo apt install -y nodejs gcc g++ make

# Install Ghost-CLI
sudo npm install ghost-cli@latest --location=global

# Create directory for Ghost installation
sudo mkdir -p /var/www/ghost

# Set directory ownership to ${linux_user} user
sudo chown ${linux_user}:${linux_user} /var/www/ghost

# Set the correct permissions
sudo chmod 775 /var/www/ghost

# Move into the Ghost directory
cd /var/www/ghost

# Check if domain ${ghost_domain} responds to any IP using dig
current_vm_ip=$(dig +short myip.opendns.com @resolver1.opendns.com)
current_domain_ip=$(dig +short ${ghost_domain})
if [ -z "$current_domain_ip" ]; then
	# Domain ${ghost_domain} does not respond to any IP, so we need to create the DNS record
	# Create the DNS record for the Ghost site using the Cloudflare API
	curl -X POST "https://api.cloudflare.com/client/v4/zones/${cloudflare_zone_id}/dns_records" \
		-H "Content-Type: application/json" \
		-H "Authorization: Bearer ${cloudflare_api_token}" \
		--data '{"type":"A","name":"${ghost_domain}","content":"'$current_vm_ip'","ttl":60,"proxied":false}'
else
	# Domain ${ghost_domain} responds to an IP, so we need to update the DNS record if it's different from the current IP
	if [ "$current_domain_ip" != "$current_vm_ip" ]; then
		# Get ${ghost_domain}'s DNS record ID
		result=$(curl -X GET "https://api.cloudflare.com/client/v4/zones/${cloudflare_zone_id}/dns_records?type=A&name=${ghost_domain}" \
			-H "Authorization: Bearer ${cloudflare_api_token}" \
			-H "Content-Type: application/json")
		# From $result, get the id of the DNS record using grep
		ghost_dns_record_id=$(echo $result | grep -Po '"id":"\K[^"]*')
		# Update the DNS record for the Ghost site using the Cloudflare API
		curl -X PUT "https://api.cloudflare.com/client/v4/zones/${cloudflare_zone_id}/dns_records/$ghost_dns_record_id" \
			-H "Content-Type: application/json" \
			-H "Authorization: Bearer ${cloudflare_api_token}" \
			--data '{"type":"A","name":"${ghost_domain}","content":"'$current_vm_ip'","ttl":60,"proxied":false}'
	fi
fi

# Install Ghost without setting it up
cd /var/www/ghost
if ! sudo -u ${linux_user} ghost install --no-setup; then
	# If it fails, try again
	sudo -u ${linux_user} ghost install --no-setup
fi

# Create config files
cp -p /var/www/ghost/.ghost-cli{,.bak}

# Append $ghost_cli_config in the middle of the JSON file /var/www/ghost/.ghost-cli
ghost_cli_config='\n  \"name\": \"${project_name}\",\n  \"running\": \"production\"'
sed -i "s/\(\"channel\": \"stable\"\)\(.*\)/\1,$ghost_cli_config\2/g" /var/www/ghost/.ghost-cli

cat > /var/www/ghost/config.production.json << EOF
{
  "url": "https://${ghost_domain}",
  "server": {
    "port": 2368,
    "host": "127.0.0.1"
  },
  "database": {
    "client": "mysql",
    "connection": {
      "host": "${mysql_host}",
      "user": "${mysql_user}",
      "password": "${mysql_password}",
      "database": "${mysql_db}"
    }
  },
  "mail": {
    "transport": "Direct"
  },
  "logging": {
    "transports": [
      "file",
      "stdout"
    ]
  },
  "process": "systemd",
  "paths": {
    "contentPath": "/var/www/ghost/content"
  }
}
EOF

# Finish Ghost setup
sudo chown -R ${linux_user}:${linux_user} /var/www/ghost
cd /var/www/ghost
sudo -u ${linux_user} ghost setup --no-prompt --sslemail ${admin_email}

echo "All went well" >> ~/all_went_well.txt

# Reboot the system
sudo reboot
